{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import absolute_import, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import product\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, cohen_kappa_score\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "First we check how much does feature selection affect the final results.\n",
    "As with MLP we are obligued to do a feature selection, we need to compare\n",
    "to see if there is a possible inconsistency later.\n",
    "\n",
    "For this we need the results of handcrafted features with and without\n",
    "feature selection for the classifiers:\n",
    "+ Decision Tree\n",
    "+ Logistic Regression\n",
    "+ Naive Bayes\n",
    "+ SVM\n",
    "\n",
    "We collect the metrics following metrics both considering monoclass\n",
    "lemmas and filtering them:\n",
    "+ Accuracy\n",
    "+ Macro Precision\n",
    "+ Macro Recall\n",
    "\n",
    "After that we also collect the Cohen's kappa score for each classifiers\n",
    "vs the ground truth. But only for the cases of lemmas with more than one class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_path = '../../results/experiment0/'\n",
    "representations = ['handcrafted', 'handcrafted_feature_selection']\n",
    "classifiers = ['decision_tree', 'log', 'naive_bayes', 'svm']\n",
    "corpora = ['sensem', 'semeval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hand_fs = []\n",
    "labels = []\n",
    "labels_count = []\n",
    "hand_fs_columns = ['classifier', 'representation', 'corpus', 'lemma', 'num_classes',\n",
    "                   'accuracy', 'macro_precision', 'macro_recall', 'kappa_score']\n",
    "\n",
    "for classifier, representation, corpus in\\\n",
    "    tqdm_notebook(product(*(classifiers, representations, corpora)),\n",
    "                  total=len(classifiers)*len(representations)*len(corpora)):\n",
    "    path = os.path.join(results_path, '%s.csv' % ('_'.join([classifier, representation, corpus])))\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    for (lemma, corpus_split), lcdf in df.groupby(['lemma', 'corpus'], sort=False):\n",
    "        if corpus_split == 'train':\n",
    "            labels, labels_count = np.unique(lcdf.true, return_counts=True)\n",
    "        \n",
    "        rdf = {'classifier': classifier,\n",
    "               'representation': representation,\n",
    "               'corpus': '%s.%s' % (corpus, corpus_split),\n",
    "               'lemma': lemma,\n",
    "               'num_classes': labels.shape[0],\n",
    "              }\n",
    "        rdf['accuracy'] = accuracy_score(lcdf.true, lcdf.prediction)\n",
    "        rdf['macro_precision'], rdf['macro_recall'], _, _ =\\\n",
    "            precision_recall_fscore_support(lcdf.true, lcdf.prediction, average='macro', labels=labels)\n",
    "        rdf['kappa_score'] = cohen_kappa_score(lcdf.true, lcdf.prediction, labels=labels)\n",
    "        hand_fs.append(rdf)\n",
    "\n",
    "hand_fs = pd.DataFrame(hand_fs, columns=hand_fs_columns)\n",
    "hand_fs.to_csv('./data/handcrafted_vs_feature_selection.csv', index=False, float_format='%.2e')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representations\n",
    "\n",
    "After finding out there is no difference using Feature Selection, we show the general metrics\n",
    "results for all the classifiers, that is:\n",
    "+ Baseline\n",
    "+ Decision Tree\n",
    "+ Logistic Regression\n",
    "+ MLP\n",
    "+ Naive Bayes\n",
    "+ SVM\n",
    "\n",
    "For this we use the following representations:\n",
    "+ Feature selection of handcrafted features\n",
    "+ Hashed features with only positive values\n",
    "+ Hashed features with positive and negative values (not valid with Naive Bayes)\n",
    "\n",
    "The first boxplot will have the representations as columns and the classifiers as rows\n",
    "with the following metrics:\n",
    "+ Accuracy\n",
    "+ Macro Precision\n",
    "+ Macro Recall\n",
    "+ PMFC\n",
    "+ RMLFC\n",
    "\n",
    "This last will show which is the best representation (or if there is any difference at all) and\n",
    "then we use the visual information to select such representation and also we discard those\n",
    "algorithms which are visually showing less performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_path = '../../results/experiment0'\n",
    "classifiers = ['baseline', 'decision_tree', 'log', 'mlp_5000', 'naive_bayes', 'svm']\n",
    "representations = ['handcrafted_feature_selection', 'hashed', 'negative_hashed']\n",
    "corpora = ['sensem', 'semeval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics_df = []\n",
    "labels = []\n",
    "labels_count = []\n",
    "columns = ['classifier', 'representation', 'lemma', 'num_classes', 'corpus', 'accuracy',\n",
    "           'macro_precision', 'macro_recall', 'pmfc', 'rmlfc', 'kappa_score', 'micro_precision', 'micro_recall',\n",
    "           'weighted_precision', 'weighted_recall']\n",
    "\n",
    "for classifier, representation, corpus in\\\n",
    "    tqdm_notebook(product(*(classifiers, representations, corpora)),\n",
    "                  total=len(classifiers)*len(representations)*len(corpora)):\n",
    "    try:\n",
    "        path = os.path.join(results_path, '%s.csv' % ('_'.join([classifier, representation, corpus])))\n",
    "        df = pd.read_csv(path)\n",
    "    except OSError:\n",
    "        continue\n",
    "    \n",
    "    for (lemma, corpus_split), lcdf in df.groupby(['lemma', 'corpus'], sort=False):\n",
    "        if corpus_split == 'train':\n",
    "            labels, labels_count = np.unique(lcdf.true, return_counts=True)\n",
    "\n",
    "        rdf = {'classifier': classifier,\n",
    "               'representation': representation,\n",
    "               'corpus': '%s.%s' % (corpus, corpus_split),\n",
    "               'lemma': lemma,\n",
    "               'num_classes': labels.shape[0],\n",
    "              }\n",
    "        rdf['accuracy'] = accuracy_score(lcdf.true, lcdf.prediction)\n",
    "        rdf['macro_precision'], rdf['macro_recall'], _, _ =\\\n",
    "            precision_recall_fscore_support(lcdf.true, lcdf.prediction, average='macro', labels=labels)\n",
    "        rdf['micro_precision'], rdf['micro_recall'], _, _ =\\\n",
    "            precision_recall_fscore_support(lcdf.true, lcdf.prediction, average='micro', labels=labels)\n",
    "        rdf['weighted_precision'], rdf['weighted_recall'], _, _ =\\\n",
    "            precision_recall_fscore_support(lcdf.true, lcdf.prediction, average='weighted', labels=labels)\n",
    "        rdf['kappa_score'] = cohen_kappa_score(lcdf.true, lcdf.prediction, labels=labels)\n",
    "\n",
    "        if labels.shape[0] > 1:\n",
    "            precision, recall, _, _ =\\\n",
    "                precision_recall_fscore_support(lcdf.true, lcdf.prediction, average=None, labels=labels)\n",
    "            mask = np.ones(recall.shape, dtype=np.bool)\n",
    "            mask[np.argmax(recall)] = False\n",
    "            rdf['pmfc'] = precision[~mask][0]\n",
    "            rdf['rmlfc'] = recall[mask].mean()\n",
    "        else:  # Ill defined metrics for such case (only label available make the metrics always 1)\n",
    "            rdf['pmfc'] = 1.0\n",
    "            rdf['rmlfc'] = 1.0\n",
    "        \n",
    "        metrics_df.append(rdf)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_df, columns=columns)\n",
    "metrics_df.to_csv('./data/experiment0_general_metrics.csv', index=False, float_format='%.2e')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Representation selection and classifiers comparison\n",
    "\n",
    "Once compared the representations, we see if any of them is visually better than the others. If\n",
    "no representation shows real improvement, we decide to go with the one that simplifies everything (for now)\n",
    "that is Hashed All Positive Features.\n",
    "\n",
    "After selecting the final representation we need to do a classifier comparison to select the classifiers\n",
    "to work with. Previous to this we filter out those classifiers that show visually worse performance\n",
    "in the previous plots (baseline and naive_bayes).\n",
    "\n",
    "This is done using two graphics:\n",
    "+ A boxplot showing the different metrics of each classifier side by side.\n",
    "+ A heatmap showing the kappa average values comparando cada clasificador contra todos los demÃ¡s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_path = '../../results/experiment0'\n",
    "classifiers = ['decision_tree', 'log', 'mlp_5000', 'svm']\n",
    "representations = ['hashed']\n",
    "corpora = ['sensem', 'semeval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions_df = {}\n",
    "\n",
    "for classifier, representation, corpus in\\\n",
    "    tqdm_notebook(product(*(classifiers, representations, corpora)),\n",
    "                  total=len(classifiers)*len(representations)*len(corpora)):\n",
    "    try:\n",
    "        path = os.path.join(results_path, '%s.csv' % ('_'.join([classifier, representation, corpus])))\n",
    "        df = pd.read_csv(path)\n",
    "    except OSError:\n",
    "        continue\n",
    "    \n",
    "    for corpus_split, cdf in df.groupby(['corpus'], sort=False):\n",
    "        if (corpus, corpus_split, 'ground_truth') not in predictions_df:\n",
    "            cdft = cdf[['lemma', 'corpus', 'true']]\n",
    "            cdft.columns = ['lemma', 'corpus_split', 'value']\n",
    "            cdft['corpus'] = corpus\n",
    "            cdft['classifier'] = 'ground_truth'            \n",
    "            predictions_df[(corpus, corpus_split, 'ground_truth')] =\\\n",
    "                cdft[['classifier', 'corpus', 'corpus_split', 'lemma', 'value']]\n",
    "\n",
    "        cdfp = cdf[['lemma', 'corpus', 'prediction']]\n",
    "        cdfp.columns = ['lemma', 'corpus_split', 'value']\n",
    "        cdfp['corpus'] = corpus\n",
    "        cdfp['classifier'] = classifier            \n",
    "        predictions_df[(corpus, corpus_split, classifier)] =\\\n",
    "            cdfp[['classifier', 'corpus', 'corpus_split', 'lemma', 'value']]\n",
    "\n",
    "predictions_df = pd.concat(predictions_df.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kappa_heatmap = []\n",
    "columns = ['lemma', 'corpus', 'corpus_split', 't1', 't2', 'kappa_score']\n",
    "\n",
    "num_labels = {}\n",
    "ground_truth_df = predictions_df[predictions_df['classifier'] == 'ground_truth']\n",
    "ground_truth_df = ground_truth_df[ground_truth_df['corpus_split'] == 'train']\n",
    "\n",
    "for (corpus, lemma), cldf in ground_truth_df.groupby(['corpus', 'lemma']):\n",
    "    num_labels[(corpus, lemma)] = cldf.value.unique().shape[0]\n",
    "\n",
    "predictions_df = predictions_df[predictions_df['corpus_split'] == 'test']\n",
    "    \n",
    "for (t1, corpus, lemma), kdf1 in\\\n",
    "    tqdm_notebook(predictions_df.groupby(['classifier', 'corpus', 'lemma'])):\n",
    "    for t2 in predictions_df['classifier'].unique():\n",
    "        kdf2 = predictions_df[\n",
    "            (predictions_df['classifier'] == t2) &\n",
    "            (predictions_df['corpus'] == corpus) &\n",
    "            (predictions_df['lemma'] == lemma)]\n",
    "        \n",
    "        if num_labels[(corpus, lemma)] > 1:\n",
    "            kappa_heatmap.append({\n",
    "                    'corpus': corpus,\n",
    "                    'lemma': lemma,\n",
    "                    't1': t1,\n",
    "                    't2': t2,\n",
    "                    'kappa_score': cohen_kappa_score(kdf1.value, kdf2.value)\n",
    "                })\n",
    "\n",
    "kappa_heatmap = pd.DataFrame(kappa_heatmap, columns=columns)\n",
    "kappa_heatmap = kappa_heatmap.groupby(['corpus', 't1', 't2'])\\\n",
    "    .agg({'kappa_score': np.mean}).reset_index()\n",
    "\n",
    "kappa_heatmap.to_csv('./data/experiment0_kappa_interclassifier.csv', index=False, float_format='%.2e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
